"""
OpenAI ChatGPT AI Engine for LLMBenchCore

This module provides an interface to the OpenAI API using the Responses API.

Setup:
1. Install the SDK: pip install openai
2. Set your API key as an environment variable:
   - Windows: set OPENAI_API_KEY=your_api_key_here
   - Linux/Mac: export OPENAI_API_KEY=your_api_key_here
   
Get your API key from: https://platform.openai.com/api-keys

The SDK documentation can be found at: https://platform.openai.com/docs
Responses API reference: https://platform.openai.com/docs/api-reference/responses
"""

import hashlib
import os
import json
import PromptImageTagging as pit


class OpenAIEngine:
  """
  OpenAI ChatGPT AI Engine class.
  
  Configuration parameters:
  - model: Model name (e.g., "gpt-5-nano")
  - reasoning: Reasoning mode:
      - False or 0: No special reasoning (standard mode)
      - "o1-preview": Use o1-preview model with extended reasoning
      - "o1-mini": Use o1-mini model (faster reasoning)
      - Integer (1-10): Reasoning effort level (for o1 models)
  - tools: Tool capabilities:
      - False: No tools available
      - True: Enable ALL built-in tools (web_search, code_interpreter)
      - List of function definitions: Enable specific custom tools
  """

  def __init__(self, model: str, reasoning=False, tools=False):
    self.model = model
    self.reasoning = reasoning
    self.tools = tools
    self.forcedFailure = False
    self.configAndSettingsHash = hashlib.sha256(model.encode() + str(reasoning).encode() +
                                                str(tools).encode()).hexdigest()

  def AIHook(self, prompt: str, structure: dict | None) -> tuple:
    """Call the OpenAI API with instance configuration."""
    result = _openai_ai_hook(prompt, structure, self.model, self.reasoning, self.tools, self)
    return result


def build_openai_input(prompt: str):
  prompt_parts = pit.parse_prompt_parts(prompt)
  has_images = any(part_type == "image" for part_type, _ in prompt_parts)
  if not has_images:
    return prompt

  content: list[dict] = []
  for part_type, part_value in prompt_parts:
    if part_type == "text":
      if part_value:
        content.append({"type": "input_text", "text": part_value})
    elif part_type == "image":
      if pit.is_url(part_value) or pit.is_data_uri(part_value):
        image_url = part_value
      else:
        image_url = pit.file_to_data_uri(pit.resolve_local_path(part_value))
      content.append({"type": "input_image", "image_url": image_url, "detail": "high"})

  return [{"role": "user", "content": content}]


def _openai_ai_hook(prompt: str, structure: dict | None, model: str, reasoning, tools,
                    engine_instance) -> tuple:
  """
    This function is called by the test runner to get the AI's response to a prompt.
    
    Prompt is the question to ask the AI.
    Structure contains the JSON schema for the expected output. If it is None, the output is just a string.
    
    There is no memory between calls to this function, the 'conversation' doesn't persist.
    
    Uses the OpenAI Responses API.
    """
  if engine_instance.forcedFailure:
    return {"error": "Forced failure"}, "Forced failure due to API instability"
  from openai import OpenAI

  try:
    # Initialize the client - it will automatically use OPENAI_API_KEY environment variable
    client = OpenAI(timeout=3600)

    # Determine model to use
    model_to_use = model

    # Override model if reasoning specifies an o1 model
    if isinstance(reasoning, str) and reasoning in ["o1-preview", "o1-mini"]:
      model_to_use = reasoning

    # Build Responses API parameters
    input_value = build_openai_input(prompt)

    response_params = {"model": model_to_use, "input": input_value, "service_tier": "flex"}

    # Add reasoning effort
    if isinstance(reasoning, int) and reasoning > 0:
      # Map 1-10 scale to low/medium/high
      if reasoning <= 3:
        response_params["reasoning"] = {"effort": "low"}
      elif reasoning <= 7:
        response_params["reasoning"] = {"effort": "medium"}
      elif reasoning == 10 and model_to_use == "gpt-5.2":
        response_params["reasoning"] = {"effort": "xhigh"}
      else:
        response_params["reasoning"] = {"effort": "high"}

      response_params["reasoning"]["summary"] = "auto"

    # Handle structured output using the text.format parameter
    if structure is not None:
      response_params["text"] = {
        "format": {
          "type": "json_schema",
          "name": "structured_response",
          "schema": structure,
          "strict": True
        }
      }

    # Add tools if specified
    if tools is True:
      # Enable built-in hosted tools
      # Note: file_search requires a vector_store, so it's excluded
      response_params["tools"] = [{
        "type": "web_search"
      }, {
        "type": "code_interpreter",
        "container": {
          "type": "auto"
        }
      }]
    elif tools and tools is not False:
      # Convert function list to OpenAI tool format if needed
      tools_list = []
      for tool in (tools if isinstance(tools, list) else [tools]):
        if isinstance(tool, dict):
          # Already in correct format
          tools_list.append(tool)
        elif callable(tool):
          # Convert Python function to tool definition
          import inspect
          sig = inspect.signature(tool)
          doc = inspect.getdoc(tool) or "No description"

          properties = {}
          required = []
          for param_name, param in sig.parameters.items():
            param_type = "string"  # Default type
            if param.annotation != inspect.Parameter.empty:
              if param.annotation == int:
                param_type = "integer"
              elif param.annotation == float:
                param_type = "number"
              elif param.annotation == bool:
                param_type = "boolean"

            properties[param_name] = {"type": param_type}
            if param.default == inspect.Parameter.empty:
              required.append(param_name)

          tool_def = {
            "type": "function",
            "name": tool.__name__,
            "description": doc,
            "parameters": {
              "type": "object",
              "properties": properties,
              "required": required
            }
          }
          tools_list.append(tool_def)

      if tools_list:
        response_params["tools"] = tools_list

    # Make the API call using Responses API with streaming
    stream = client.responses.create(stream=True, timeout=3600, **response_params)

    chainOfThought = ""
    output_text = ""
    current_reasoning_line = ""

    # Process streaming events
    for event in stream:
      event_type = event.type

      # Handle reasoning summary deltas - print line by line as they arrive
      if event_type == "response.reasoning_summary_text.delta":
        delta = event.delta
        current_reasoning_line += delta
        # Print complete lines as they arrive
        while "\n" in current_reasoning_line:
          line, current_reasoning_line = current_reasoning_line.split("\n", 1)
          print(f"Thinking: {line}", flush=True)
          chainOfThought += line + "\n"

      # Handle reasoning summary done - flush any remaining text
      elif event_type == "response.reasoning_summary_text.done":
        if current_reasoning_line:
          print(f"Thinking: {current_reasoning_line}", flush=True)
          chainOfThought += current_reasoning_line
          current_reasoning_line = ""

      # Handle output text deltas - accumulate silently
      elif event_type == "response.output_text.delta":
        output_text += event.delta

      # Handle completion
      elif event_type == "response.completed":
        # Final response is available if needed
        pass

    # Strip trailing newline from chain of thought if present
    chainOfThought = chainOfThought.rstrip("\n")

    #print(output_text)

    # Extract content
    if structure is not None:
      # Parse JSON response
      if output_text:
        return json.loads(output_text), chainOfThought
      return {}, chainOfThought
    else:
      # Return text response
      return output_text or "", chainOfThought

  except json.JSONDecodeError:
    print(
      "Error decoding JSON response. OpenAI has schema validation that's failing. Consider the whole service down when this is encountered."
    )
    engine_instance.forcedFailure = True
    return {"unacceptableFailure": True}, ""  # to ensure we don't retry.
  except Exception as e:
    print(f"Error calling OpenAI API: {e}")

    # Check for content policy violation
    from ContentViolationHandler import is_content_violation_openai
    if is_content_violation_openai(e):
      print("CONTENT VIOLATION DETECTED (OpenAI)")
      if structure is not None:
        return {"__content_violation__": True, "reason": str(e)}, f"Content violation: {e}"
      else:
        return "__content_violation__", f"Content violation: {e}"

    # Return appropriate empty response based on structure
    if structure is not None:
      return {}
    else:
      return ""
